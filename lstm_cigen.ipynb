{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSTM as Autoencoder for Compute FCI\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: Kernel en pc Ivan = env_nvn_rag(python 3.10.13) ya que tiene pytorch cuda\n",
    "      Kernel en pc Sandra ="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "import clean\n",
    "import numpy as np\n",
    "import torch\n",
    "from pylab import rcParams\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import rc\n",
    "from torch import nn, optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "get the determinants and format them into a numpy array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        1., 1., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [1., 1., 1., 1., 0., 0., 1., 1., 1., 1., 0., 0., 1., 1., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]], dtype=float32)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "name='psi_det'\n",
    "so_vectors=clean.clean(name)\n",
    "so_vectors=np.array(so_vectors,dtype=np.float32)\n",
    "display(so_vectors[:3])\n",
    "so_vectors=torch.tensor(so_vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "RANDOM_SEED = 42\n",
    "np.random.seed(RANDOM_SEED)\n",
    "torch.manual_seed(RANDOM_SEED)\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "\n",
    "  def __init__(self, seq_len, n_features, embedding_dim=64):\n",
    "    super(Encoder, self).__init__()\n",
    "    self.seq_len, self.n_features = seq_len, n_features\n",
    "    self.embedding_dim, self.hidden_dim = embedding_dim, 2 * embedding_dim\n",
    "\n",
    "    self.rnn1 = nn.LSTM(\n",
    "      input_size=n_features,\n",
    "      hidden_size=self.hidden_dim,\n",
    "      num_layers=1,\n",
    "      batch_first=True\n",
    "    )\n",
    "    \n",
    "    self.rnn2 = nn.LSTM(\n",
    "      input_size=self.hidden_dim,\n",
    "      hidden_size=embedding_dim,\n",
    "      num_layers=1,\n",
    "      batch_first=True\n",
    "    )\n",
    "\n",
    "  def forward(self, x):\n",
    "    x, (_, _) = self.rnn1(x)\n",
    "    x, (hidden_n, _) = self.rnn2(x)\n",
    "    return x\n",
    "  \n",
    "class Decoder(nn.Module):\n",
    "  def __init__(self, seq_len,input_dim=64, n_features=8):\n",
    "    super(Decoder, self).__init__()\n",
    "\n",
    "    self.seq_len, self.input_dim = seq_len, input_dim\n",
    "    self.hidden_dim, self.n_features = 2 * input_dim, n_features\n",
    "\n",
    "    self.rnn1 = nn.LSTM(\n",
    "      input_size=input_dim,\n",
    "      hidden_size=input_dim,\n",
    "      num_layers=1,\n",
    "      batch_first=True\n",
    "    )\n",
    "\n",
    "    self.rnn2 = nn.LSTM(\n",
    "      input_size=input_dim,\n",
    "      hidden_size=self.hidden_dim,\n",
    "      num_layers=1,\n",
    "      batch_first=True\n",
    "    )\n",
    "\n",
    "    self.output_layer = nn.Linear(self.hidden_dim, n_features)\n",
    "  def forward(self, x):\n",
    "    x, (hidden_n, cell_n) = self.rnn1(x)\n",
    "    x, (hidden_n, cell_n) = self.rnn2(x)\n",
    "    x = x.reshape((-1,self.seq_len, self.hidden_dim))  \n",
    "    x = F.normalize(torch.abs(self.output_layer(x)), p=1, dim=1)\n",
    "\n",
    "    return x \n",
    "  \n",
    "\n",
    "class RecurrentAutoencoder(nn.Module):\n",
    "    def __init__(self, seq_len,ne, n_features,embedding_dim=64):\n",
    "        super(RecurrentAutoencoder, self).__init__()\n",
    "        self.encoder = Encoder(seq_len, n_features, embedding_dim).to(device)\n",
    "        self.decoder = Decoder(seq_len, embedding_dim, n_features).to(device)\n",
    "        self.ne=ne\n",
    "\n",
    "      \n",
    "    #my own function to apli to the final output of decoder\n",
    "    def electron_constriction(self, x): #use [-1,:,-1] to get the last value of the last sequence\n",
    "        x2=x[-1,:  ,-1]\n",
    "        x2=torch.abs(x2)\n",
    "        zero_vector=torch.zeros(x2.shape,dtype=torch.float32)\n",
    "\n",
    "        for i in range (self.ne//2):\n",
    "          #----------------------------constriction for even  positions (electrons in alpha orbitals)---------------------\n",
    "          random_value=torch.rand(1).to(device)\n",
    "          #cumsum of odd values\n",
    "          odd_values=torch.cumsum(x2[::2],0)\n",
    "          random_values=random_value*odd_values[-1] #random values in range of cumsum\n",
    "\n",
    "\n",
    "          #get the index of the closest value to the random value using argmax\n",
    "          mask=odd_values>=random_values\n",
    "          Index_where_mask_is_true = (torch.nonzero(mask, as_tuple=True)[0])*2  # is like argmax but for boolean in pytorch\n",
    "          jump=Index_where_mask_is_true[0]\n",
    "\n",
    "          x2[jump]=0  #set to 0 the prbability in x2\n",
    "          zero_vector[jump]=1 #set to 1 the position of the jump in the vector x\n",
    "\n",
    "\n",
    "          #----------------------------constriction for odd  positions (electrons in beta orbitals)---------------------\n",
    "          random_value=torch.rand(1).to(device)\n",
    "          even_values=torch.cumsum(x2[1::2],0)\n",
    "          random_values=random_value*even_values[-1] #random values in range of cumsum\n",
    "\n",
    "          #get the index of the closest value to the random value using argmax\n",
    "          mask=even_values>=random_values\n",
    "          Index_where_mask_is_true = (torch.nonzero(mask, as_tuple=True)[0])*2+1  # is like argmax but for boolean in pytorch\n",
    "          jump=Index_where_mask_is_true[0]\n",
    "\n",
    "          x2[jump]=0  #set to 0 the prbability in x2\n",
    "          zero_vector[jump]=1 #set to 1 the position of the jump in the vector x\n",
    "          \n",
    "        if torch.count_nonzero(zero_vector)!=self.ne:\n",
    "          print('error numero de electrones en vector es:',torch.count_nonzero(zero_vector))\n",
    "\n",
    "        x[-1,:,-1]=zero_vector  #set the vector with the constriction of electrons\n",
    "\n",
    "        #x to device cuda\n",
    "        return x.to(device)\n",
    "  \n",
    "\n",
    "    def forward(self, x):    \n",
    "        x = self.encoder(x)\n",
    "        x = self.decoder(x)\n",
    "        x=self.electron_constriction(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TimeSeriesDataset(Dataset):\n",
    "    def __init__(self, data, seq_len):\n",
    "        self.data = data\n",
    "        self.seq_len = seq_len\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.data[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, train_loader,  n_epochs,lr):\n",
    "  optimizer = torch.optim.Adam(model.parameters(), lr)\n",
    "  criterion = nn.L1Loss(reduction='mean').to(device) #analizies MAE between prediction and target\n",
    "  history = dict(train=[], val=[])\n",
    "\n",
    "  for epoch in range(1, n_epochs + 1):\n",
    "    model = model.train()\n",
    "    print(f'\\rÉpoca: {epoch}', end='', flush=True)\n",
    "    train_losses = []\n",
    "    for seq_true in train_loader:\n",
    "      optimizer.zero_grad()\n",
    "      seq_true = seq_true.to(device)\n",
    "      seq_pred = model(seq_true)\n",
    "      loss = criterion(seq_pred, seq_true)\n",
    "      loss.backward()\n",
    "      optimizer.step()\n",
    "      train_losses.append(loss.item())\n",
    "    model = model.eval()\n",
    "   \n",
    "  return model.eval(), history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_len=26\n",
    "features=1\n",
    "num_samples = len(so_vectors) \n",
    "tensor_data = so_vectors[:num_samples * seq_len]\n",
    "tensor_data = tensor_data.reshape((num_samples, seq_len, features))\n",
    "indices = torch.randperm(num_samples)\n",
    "tensor_data = tensor_data[indices]\n",
    "train_dataset = TimeSeriesDataset(tensor_data, seq_len)\n",
    "batch_size=64\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n",
      "Época: 10"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)\n",
    "model = RecurrentAutoencoder(26,10, 1,64)\n",
    "model = model.to(device)\n",
    "model, history = train_model(\n",
    "  model, \n",
    "  train_loader,  \n",
    "  n_epochs=10,\n",
    "  lr=0.001\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), 'recurrent_autoencoder.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the entire state_dict\n",
    "state_dict = torch.load('recurrent_autoencoder.pth')\n",
    "device = torch.device(\"cpu\")\n",
    "# Initialize the decoder\n",
    "decoder = Decoder(seq_len=26, input_dim=64, n_features=1).to(device)\n",
    "\n",
    "# Load only the decoder weights\n",
    "decoder_state_dict = {k.replace('decoder.', ''): v for k, v in state_dict.items() if k.startswith('decoder.')}\n",
    "decoder.load_state_dict(decoder_state_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generation of new determinants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generation(decoder,Ne,dets_train):\n",
    "    random_data=torch.randn(1, 26, 64)\n",
    "    decoded_output = decoder(random_data)\n",
    "    visible_probs = decoded_output.detach().numpy()\n",
    "    #formated to 4 decimals\n",
    "    #print('probabilidades visibles',np.around(visible_probs[0],5))\n",
    "    #print the 5 orbitals with the highest probabilities\n",
    "    a=np.argsort(visible_probs,axis=1)\n",
    "    #print(visible_probs)\n",
    "    print('los 5 orbitales con mayor probabilidad son',a[0][-5:])\n",
    "\n",
    "    #random num to singlet or doublet\n",
    "    exitations=0\n",
    "    random_exitation= np.random.randint(1)\n",
    "    if random_exitation>0.5:\n",
    "        exitations=1\n",
    "    else:\n",
    "        exitations=2\n",
    "\n",
    "    print('numero de excitaciones',exitations)\n",
    "\n",
    "    random_number_det=np.random.randint(len(dets_train))\n",
    "    det=dets_train[random_number_det]   #choose a random determinant from the training set\n",
    "    print('Tensor 1',det)\n",
    "\n",
    "    #position of the occupied and unoccupied orbitals\n",
    "    occupied_orbitals=np.where(det==1)[0]\n",
    "    unoccupied_orbitals=np.where(det==0)[0]\n",
    "\n",
    "    #compute the conditional probabilities for a change of 1 electron in a occupied orbital to a unoccupied orbital\n",
    "    jumps=np.zeros((len(occupied_orbitals),len(unoccupied_orbitals)))\n",
    "    for j in range(len(occupied_orbitals)):\n",
    "        for k in range(len(unoccupied_orbitals)):\n",
    "            #probabilities of the jumps, (1 - prob of the occupied orbital being ocuped)*(prob of the unoccupied orbital being ocuped)\n",
    "            #this is the same as: probability of the occupied orbital being unocuped * probability of the unoccupied orbital being ocuped\n",
    "            jumps[j][k]=(1-visible_probs[0][occupied_orbitals[j]])*visible_probs[0][unoccupied_orbitals[k]]\n",
    "\n",
    "\n",
    "    #sample the jumps\n",
    "    new_det = det.clone() #copy the determinant to change it\n",
    "    saved_jumps=[]\n",
    "    for i in range(exitations):\n",
    "        c=0 #acumulative sum of probabilities\n",
    "        #p=np.random.random()*np.sum(jumps) #random number between 0 and the sum of all the probabilities in the jumps matrix\n",
    "        p=random.random()*np.sum(jumps) #creo que funciona mejor en el paralelismo\n",
    "        \n",
    "        found_change=False\n",
    "\n",
    "        for j in range(len(occupied_orbitals)):\n",
    "            for k in range(len(unoccupied_orbitals)):\n",
    "                c+=jumps[j][k]\n",
    "                \n",
    "                #if the random number is less or equal to the acumulative sum of probabilities, then we change the determinant\n",
    "                # the restriction jumps[j][k]!=0 is to avoid to choose the same occupied and unoccupied orbital, that previously was changed to 0\n",
    "                #for example (j=7 y k= 30 suma acumulada 112.07108543752757) y  (j=7 y k =31 suma acumulada 112.07108543752757), the probabilities \n",
    "                #are the same, it means that the occupied orbital 7 was changed to 0, and the unoccupied orbital 30 was changed to 0, so we cannot choose\n",
    "                if p<=c and jumps[j][k]!=0: \n",
    "                    #print('cambio de orbital',occupied_orbitals[j],'por',unoccupied_orbitals[k])\n",
    "                \n",
    "                    #check if the occupied and unoccupied orbitals have the same spin (alpha electron in alpha orbital or beta electron in beta orbital)\n",
    "                    #if (j%2==0 and k%2==0) or (j%2!=0 and k%2!=0):\n",
    "                    if (occupied_orbitals[j]%2==0 and unoccupied_orbitals[k]%2==0) or (occupied_orbitals[j]%2!=0 and unoccupied_orbitals[k]%2!=0):\n",
    "                        #change the determinant\n",
    "                        new_det[occupied_orbitals[j]]=0\n",
    "                        new_det[unoccupied_orbitals[k]]=1\n",
    "\n",
    "                        #change the probabilities for this orbitals to 0, to avoid to choose them again\n",
    "                        #jumps[j][:]=0\n",
    "                        jumps[j]=0\n",
    "                        jumps[:,k]=0\n",
    "                        #print('las probabilidades de los jumps intermedias son',jumps)\n",
    "\n",
    "                        found_change=True\n",
    "\n",
    "                        #save the jumps\n",
    "                        saved_jumps.append([occupied_orbitals[j],unoccupied_orbitals[k]])\n",
    "                        \n",
    "                        break\n",
    "\n",
    "            if found_change:\n",
    "                break\n",
    "        if not found_change:\n",
    "            print('no se encontro cambio, es posible que haya un error en el algoritmo******************************')\n",
    "            continue\n",
    "\n",
    "    return new_det\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "los 5 orbitales con mayor probabilidad son [[24]\n",
      " [ 8]\n",
      " [25]\n",
      " [10]\n",
      " [ 9]]\n",
      "numero de excitaciones 2\n",
      "Tensor 1 tensor([1., 1., 1., 0., 1., 1., 0., 1., 1., 1., 0., 0., 0., 0., 1., 0., 0., 0.,\n",
      "        0., 0., 0., 1., 0., 0., 0., 0.])\n",
      "Tensor 2 tensor([1., 1., 1., 1., 1., 1., 0., 0., 1., 0., 0., 1., 0., 0., 1., 0., 0., 0.,\n",
      "        0., 0., 0., 1., 0., 0., 0., 0.])\n"
     ]
    }
   ],
   "source": [
    "new_det_generated=generation(decoder,10,so_vectors)\n",
    "print('Tensor 2',new_det_generated)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
